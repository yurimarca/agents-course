# Hagamos Fine-Tuning de Tu Modelo para Llamadas a Funciones

Ahora estamos listos para hacer fine-tuning de nuestro primer modelo para llamadas a funciones 游댠.

## 쮺칩mo entrenamos nuestro modelo para llamadas a funciones?

> Respuesta: Necesitamos **datos**

Un proceso de entrenamiento de modelo se puede dividir en 3 pasos:

1. **El modelo es pre-entrenado con una gran cantidad de datos**. El resultado de ese paso es un **modelo pre-entrenado**. Por ejemplo, [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b). Es un modelo base y solo sabe c칩mo **predecir el siguiente token sin fuertes capacidades de seguimiento de instrucciones**.

2. Para ser 칰til en un contexto de chat, el modelo luego necesita ser **ajustado (fine-tuned)** para seguir instrucciones. En este paso, puede ser entrenado por los creadores del modelo, la comunidad de c칩digo abierto, t칰 o cualquier persona. Por ejemplo, [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) es un modelo ajustado para instrucciones por el equipo de Google detr치s del proyecto Gemma.

3. El modelo puede luego **alinearse** con las preferencias del creador. Por ejemplo, un modelo de chat de servicio al cliente que nunca debe ser grosero con los clientes.

Usualmente un producto completo como Gemini o Mistral **pasar치 por los 3 pasos**, mientras que los modelos que puedes encontrar en Hugging Face han completado uno o m치s pasos de este entrenamiento.

En este tutorial, construiremos un modelo de llamadas a funciones basado en [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it). Elegimos el modelo ajustado [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) en lugar del modelo base [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b) porque el modelo ajustado ha sido mejorado para nuestro caso de uso.

Comenzar desde el modelo pre-entrenado **requerir칤a m치s entrenamiento para aprender a seguir instrucciones, chatear Y hacer llamadas a funciones**.

Al comenzar desde el modelo ajustado para instrucciones, **minimizamos la cantidad de informaci칩n que nuestro modelo necesita aprender**.

## LoRA (Adaptaci칩n de Bajo Rango de Modelos de Lenguaje Grandes)

LoRA es una t칠cnica de entrenamiento popular y ligera que **reduce significativamente el n칰mero de par치metros a entrenar**.

Funciona **insertando un n칰mero menor de nuevos pesos(weights) como un adaptador en el modelo para entrenar**. Esto hace que el entrenamiento con LoRA sea mucho m치s r치pido, eficiente en memoria y produzca pesos(weights) de modelo m치s peque침os (unos cientos de MB), que son m치s f치ciles de almacenar y compartir.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/blog_multi-lora-serving_LoRA.gif" alt="Inferencia LoRA" width="50%"/>

LoRA funciona a침adiendo pares de matrices de descomposici칩n de rango a las capas del Transformer, t칤picamente centr치ndose en las capas lineales. Durante el entrenamiento, "congelaremos" el resto del modelo y solo actualizaremos los pesos de esos adaptadores reci칠n a침adidos.

Al hacerlo, el n칰mero de **par치metros** que necesitamos entrenar disminuye considerablemente ya que solo necesitamos actualizar los pesos del adaptador.

Durante la inferencia, la entrada se pasa al adaptador y al modelo base, o estos pesos del adaptador pueden fusionarse con el modelo base, lo que resulta en ninguna sobrecarga adicional de latencia.

LoRA es particularmente 칰til para adaptar modelos de lenguaje **grandes** a tareas o dominios espec칤ficos mientras se mantienen manejables los requisitos de recursos. Esto ayuda a reducir la memoria **requerida** para entrenar un modelo.

Si quieres aprender m치s sobre c칩mo funciona LoRA, deber칤as consultar este [tutorial](https://huggingface.co/learn/nlp-course/chapter11/4?fw=pt).

## Fine-Tuning de un Modelo para Llamadas a Funciones

Puedes acceder al notebook del tutorial 游녤 [aqu칤](https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb).

Luego, haz clic en [![Abrir En Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb) para poder ejecutarlo en un Notebook de Colab.
