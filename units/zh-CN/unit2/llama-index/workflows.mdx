# åœ¨ LlamaIndex ä¸­åˆ›å»ºæ™ºèƒ½å·¥ä½œæµ

LlamaIndex ä¸­çš„å·¥ä½œæµæä¾›äº†ä¸€ç§ç»“æ„åŒ–æ–¹å¼æ¥å°†ä»£ç ç»„ç»‡æˆå¯ç®¡ç†çš„é¡ºåºæ­¥éª¤ã€‚

è¿™ç§å·¥ä½œæµé€šè¿‡å®šä¹‰ç”±`äº‹ä»¶ï¼ˆEventsï¼‰`è§¦å‘çš„`æ­¥éª¤ï¼ˆStepsï¼‰`æ¥åˆ›å»ºï¼Œè¿™äº›æ­¥éª¤æœ¬èº«ä¹Ÿä¼šå‘å‡º`äº‹ä»¶`æ¥è§¦å‘åç»­æ­¥éª¤ã€‚
è®©æˆ‘ä»¬çœ‹çœ‹ Alfred å±•ç¤ºçš„ç”¨äº RAG ä»»åŠ¡çš„ LlamaIndex å·¥ä½œæµã€‚

![å·¥ä½œæµç¤ºæ„å›¾](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflows.png)

**å·¥ä½œæµå…·æœ‰ä»¥ä¸‹å…³é”®ä¼˜åŠ¿ï¼š**

- å°†ä»£ç æ¸…æ™°åœ°ç»„ç»‡ä¸ºç¦»æ•£æ­¥éª¤
- äº‹ä»¶é©±åŠ¨æ¶æ„å®ç°çµæ´»æ§åˆ¶æµ
- æ­¥éª¤é—´ç±»å‹å®‰å…¨çš„é€šä¿¡
- å†…ç½®çŠ¶æ€ç®¡ç†
- æ”¯æŒç®€å•å’Œå¤æ‚çš„æ™ºèƒ½ä½“äº¤äº’

æ­£å¦‚æ‚¨å¯èƒ½çŒœåˆ°çš„ï¼Œ**å·¥ä½œæµåœ¨ä¿æŒå¯¹æ•´ä½“æµç¨‹æ§åˆ¶çš„åŒæ—¶ï¼Œå®ç°äº†æ™ºèƒ½ä½“çš„è‡ªä¸»æ€§ä¹‹é—´çš„å®Œç¾å¹³è¡¡ã€‚**

ç°åœ¨è®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•è‡ªå·±åˆ›å»ºå·¥ä½œæµï¼

## åˆ›å»ºå·¥ä½œæµ

<Tip>
æ‚¨å¯ä»¥é€šè¿‡ <a href="https://huggingface.co/agents-course/notebooks/blob/main/unit2/llama-index/workflows.ipynb" target="_blank">è¿™ä¸ªç¬”è®°æœ¬</a> ä¸­çš„ä»£ç è¿›è¡Œå®è·µï¼Œå¯ä½¿ç”¨ Google Colab è¿è¡Œã€‚
</Tip>

### åŸºç¡€å·¥ä½œæµåˆ›å»º

<details>
<summary>å®‰è£…å·¥ä½œæµåŒ…</summary>
å¦‚ [LlamaHub ç« èŠ‚](llama-hub) ä»‹ç»çš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…å·¥ä½œæµåŒ…ï¼š

```python
pip install llama-index-utils-workflow
```
</details>

æˆ‘ä»¬å¯ä»¥é€šè¿‡å®šä¹‰ä¸€ä¸ªç»§æ‰¿è‡ª `Workflow` çš„ç±»å¹¶ç”¨ `@step` è£…é¥°ä½ çš„å‡½æ•°æ¥åˆ›å»ºä¸€ä¸ªå•æ­¥å·¥ä½œæµã€‚
æˆ‘ä»¬è¿˜éœ€è¦æ·»åŠ  `StartEvent` å’Œ `StopEvent`ï¼Œå®ƒä»¬æ˜¯ç”¨äºæŒ‡ç¤ºå·¥ä½œæµå¼€å§‹å’Œç»“æŸçš„ç‰¹æ®Šäº‹ä»¶ã€‚

```python
from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step

class MyWorkflow(Workflow):
    @step
    async def my_step(self, ev: StartEvent) -> StopEvent:
        # do something here
        return StopEvent(result="Hello, world!")


w = MyWorkflow(timeout=10, verbose=False)
result = await w.run()
```

å¦‚æ‚¨æ‰€è§ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨â€œw.run()â€æ¥è¿è¡Œå·¥ä½œæµç¨‹ã€‚

### è¿æ¥å¤šä¸ªæ­¥éª¤

ä¸ºäº†è¿æ¥å¤šä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬**åˆ›å»ºåœ¨æ­¥éª¤ä¹‹é—´ä¼ è¾“æ•°æ®çš„è‡ªå®šä¹‰äº‹ä»¶**ã€‚
ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ·»åŠ ä¸€ä¸ªåœ¨æ­¥éª¤ä¹‹é—´ä¼ é€’çš„â€œäº‹ä»¶â€ï¼Œå¹¶å°†ç¬¬ä¸€æ­¥çš„è¾“å‡ºä¼ è¾“åˆ°ç¬¬äºŒæ­¥ã€‚

```python
from llama_index.core.workflow import Event

class ProcessingEvent(Event):
    intermediate_result: str

class MultiStepWorkflow(Workflow):
    @step
    async def step_one(self, ev: StartEvent) -> ProcessingEvent:
        # Process initial data
        return ProcessingEvent(intermediate_result="Step 1 complete")

    @step
    async def step_two(self, ev: ProcessingEvent) -> StopEvent:
        # Use the intermediate result
        final_result = f"Finished processing: {ev.intermediate_result}"
        return StopEvent(result=final_result)

w = MultiStepWorkflow(timeout=10, verbose=False)
result = await w.run()
result
```

ç±»å‹æç¤ºåœ¨è¿™é‡Œå¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒå¯ä»¥ç¡®ä¿å·¥ä½œæµæ­£ç¡®æ‰§è¡Œã€‚è®©æˆ‘ä»¬æŠŠäº‹æƒ…å¤æ‚åŒ–ä¸€ç‚¹å§ï¼

### å¾ªç¯å’Œåˆ†æ”¯

ç±»å‹æç¤ºæ˜¯å·¥ä½œæµä¸­æœ€å¼ºå¤§çš„éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒå…è®¸æˆ‘ä»¬åˆ›å»ºåˆ†æ”¯ã€å¾ªç¯å’Œè¿æ¥ä»¥ä¿ƒè¿›æ›´å¤æ‚çš„å·¥ä½œæµã€‚

è®©æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä½¿ç”¨è”åˆè¿ç®—ç¬¦ `|` **åˆ›å»ºå¾ªç¯** çš„ç¤ºä¾‹ã€‚
åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ° `LoopEvent` è¢«ä½œä¸ºæ­¥éª¤çš„è¾“å…¥ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºè¾“å‡ºè¿”å›ã€‚

```python
from llama_index.core.workflow import Event
import random


class ProcessingEvent(Event):
    intermediate_result: str


class LoopEvent(Event):
    loop_output: str


class MultiStepWorkflow(Workflow):
    @step
    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:
        if random.randint(0, 1) == 0:
            print("Bad thing happened")
            return LoopEvent(loop_output="Back to step one.")
        else:
            print("Good thing happened")
            return ProcessingEvent(intermediate_result="First step complete.")

    @step
    async def step_two(self, ev: ProcessingEvent) -> StopEvent:
        # Use the intermediate result
        final_result = f"Finished processing: {ev.intermediate_result}"
        return StopEvent(result=final_result)


w = MultiStepWorkflow(verbose=False)
result = await w.run()
result
```

### ç»˜åˆ¶å·¥ä½œæµç¨‹

æˆ‘ä»¬è¿˜å¯ä»¥ç»˜åˆ¶å·¥ä½œæµç¨‹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ `draw_all_possible_flows` å‡½æ•°æ¥ç»˜åˆ¶å·¥ä½œæµç¨‹ã€‚è¿™ä¼šå°†å·¥ä½œæµç¨‹å­˜å‚¨åœ¨ HTML æ–‡ä»¶ä¸­ã€‚

```python
from llama_index.utils.workflow import draw_all_possible_flows

w = ... # as defined in the previous section
draw_all_possible_flows(w, "flow.html")
```

![å·¥ä½œæµç¨‹å›¾](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflow-draw.png)

è¯¾ç¨‹ä¸­æˆ‘ä»¬å°†ä»‹ç»æœ€åä¸€ä¸ªå¾ˆé…·çš„æŠ€å·§ï¼Œå³å‘å·¥ä½œæµæ·»åŠ çŠ¶æ€çš„èƒ½åŠ›ã€‚

### çŠ¶æ€ç®¡ç†

å½“æ‚¨æƒ³è¦è·Ÿè¸ªå·¥ä½œæµçš„çŠ¶æ€æ—¶ï¼ŒçŠ¶æ€ç®¡ç†éå¸¸æœ‰ç”¨ï¼Œè¿™æ ·æ¯ä¸ªæ­¥éª¤éƒ½å¯ä»¥è®¿é—®ç›¸åŒçš„çŠ¶æ€ã€‚
æˆ‘ä»¬å¯ä»¥åœ¨æ­¥éª¤å‡½æ•°ä¸­çš„å‚æ•°ä¸Šä½¿ç”¨â€œä¸Šä¸‹æ–‡â€ç±»å‹æç¤ºæ¥å®ç°è¿™ä¸€ç‚¹ã€‚

```python
from llama_index.core.workflow import Context, StartEvent, StopEvent


@step
async def query(self, ctx: Context, ev: StartEvent) -> StopEvent:
    # å­˜å‚¨åœ¨ä¸Šä¸‹æ–‡ä¸­
    await ctx.set("query", "What is the capital of France?")

    # æ ¹æ®ä¸Šä¸‹æ–‡å’Œäº‹ä»¶åšæŸäº‹
    val = ...

    # ä»ä¸Šä¸‹æ–‡ä¸­æ£€ç´¢
    query = await ctx.get("query")

    return StopEvent(result=result)
```

å¤ªæ£’äº†ï¼ç°åœ¨æ‚¨çŸ¥é“å¦‚ä½•åœ¨ LlamaIndex ä¸­åˆ›å»ºåŸºæœ¬å·¥ä½œæµäº†ï¼

<Tip>å·¥ä½œæµè¿˜æœ‰ä¸€äº›æ›´å¤æ‚çš„ç»†å¾®å·®åˆ«ï¼Œæ‚¨å¯ä»¥åœ¨<a href="https://docs.llamaindex.ai/en/stable/understanding/workflows/">LlamaIndex æ–‡æ¡£</a>ä¸­äº†è§£ã€‚</Tip>

ä½†æ˜¯ï¼Œè¿˜æœ‰å¦ä¸€ç§åˆ›å»ºå·¥ä½œæµçš„æ–¹æ³•ï¼Œå®ƒä¾èµ–äº `AgentWorkflow` ç±»ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨å®ƒæ¥åˆ›å»ºå¤šæ™ºèƒ½ä½“å·¥ä½œæµã€‚

## ä½¿ç”¨å¤šæ™ºèƒ½ä½“å·¥ä½œæµè‡ªåŠ¨åŒ–å·¥ä½œæµ

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨**`AgentWorkflow` ç±»æ¥åˆ›å»ºå¤šæ™ºèƒ½ä½“å·¥ä½œæµ**ï¼Œè€Œæ— éœ€æ‰‹åŠ¨åˆ›å»ºå·¥ä½œæµã€‚
`AgentWorkflow` ä½¿ç”¨å·¥ä½œæµæ™ºèƒ½ä½“ï¼Œå…è®¸æ‚¨åˆ›å»ºä¸€ä¸ªæˆ–å¤šä¸ªæ™ºèƒ½ä½“çš„ç³»ç»Ÿï¼Œè¿™äº›æ™ºèƒ½ä½“å¯ä»¥æ ¹æ®å…¶ä¸“é—¨åŠŸèƒ½è¿›è¡Œåä½œå¹¶ç›¸äº’äº¤æ¥ä»»åŠ¡ã€‚
è¿™å¯ä»¥æ„å»ºå¤æ‚çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå…¶ä¸­ä¸åŒçš„æ™ºèƒ½ä½“å¤„ç†ä»»åŠ¡çš„ä¸åŒæ–¹é¢ã€‚
æˆ‘ä»¬å°†ä»`llama_index.core.agent.workflow` å¯¼å…¥æ™ºèƒ½ä½“ç±»ï¼Œè€Œä¸æ˜¯ä»`llama_index.core.agent` å¯¼å…¥ç±»ã€‚
åœ¨`AgentWorkflow` æ„é€ å‡½æ•°ä¸­ï¼Œå¿…é¡»å°†ä¸€ä¸ªæ™ºèƒ½ä½“æŒ‡å®šä¸ºæ ¹æ™ºèƒ½ä½“ã€‚
å½“ç”¨æˆ·æ¶ˆæ¯ä¼ å…¥æ—¶ï¼Œå®ƒé¦–å…ˆè¢«è·¯ç”±åˆ°æ ¹æ™ºèƒ½ä½“ã€‚

ç„¶åæ¯ä¸ªæ™ºèƒ½ä½“å¯ä»¥ï¼š

- ä½¿ç”¨ä»–ä»¬çš„å·¥å…·ç›´æ¥å¤„ç†è¯·æ±‚
- äº¤æ¥ç»™æ›´é€‚åˆè¯¥ä»»åŠ¡çš„å¦ä¸€ä¸ªæ™ºèƒ½ä½“
- å‘ç”¨æˆ·è¿”å›å“åº”

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åˆ›å»ºå¤šæ™ºèƒ½ä½“å·¥ä½œæµã€‚

```python
from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# å®šä¹‰ä¸€äº›å·¥å…·
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")

# æˆ‘ä»¬å¯ä»¥ç›´æ¥ä¼ é€’å‡½æ•°ï¼Œè€Œæ— éœ€ FunctionToolâ€”â€”fn/docstring ä¼šè¢«è§£æä¸ºåç§°/æè¿°
multiply_agent = ReActAgent(
    name="multiply_agent",
    description="Is able to multiply two integers",
    system_prompt="A helpful assistant that can use a tool to multiply numbers.",
    tools=[multiply],
    llm=llm,
)

addition_agent = ReActAgent(
    name="add_agent",
    description="Is able to add two integers",
    system_prompt="A helpful assistant that can use a tool to add numbers.",
    tools=[add],
    llm=llm,
)

# åˆ›å»ºå·¥ä½œæµ
workflow = AgentWorkflow(
    agents=[multiply_agent, addition_agent],
    root_agent="multiply_agent",
)

# è¿è¡Œç³»ç»Ÿ
response = await workflow.run(user_msg="Can you add 5 and 3?")
```

æ™ºèƒ½ä½“å·¥å…·è¿˜å¯ä»¥ä¿®æ”¹æˆ‘ä»¬å‰é¢æåˆ°çš„å·¥ä½œæµçŠ¶æ€ã€‚åœ¨å¯åŠ¨å·¥ä½œæµä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥æä¾›ä¸€ä¸ªå¯ä¾›æ‰€æœ‰æ™ºèƒ½ä½“ä½¿ç”¨çš„åˆå§‹çŠ¶æ€å­—å…¸ã€‚
çŠ¶æ€å­˜å‚¨åœ¨å·¥ä½œæµä¸Šä¸‹æ–‡çš„ state é”®ä¸­ã€‚å®ƒå°†è¢«æ³¨å…¥åˆ° state_prompt ä¸­ï¼Œä»¥å¢å¼ºæ¯ä¸ªæ–°ç”¨æˆ·æ¶ˆæ¯ã€‚

è®©æˆ‘ä»¬é€šè¿‡ä¿®æ”¹å‰é¢çš„ç¤ºä¾‹æ¥æ³¨å…¥ä¸€ä¸ªè®¡æ•°å™¨æ¥è®¡æ•°å‡½æ•°è°ƒç”¨ï¼š

```python
from llama_index.core.workflow import Context

# å®šä¹‰ä¸€äº›å·¥å…·
async def add(ctx: Context, a: int, b: int) -> int:
    """Add two numbers."""
    # update our count
    cur_state = await ctx.get("state")
    cur_state["num_fn_calls"] += 1
    await ctx.set("state", cur_state)

    return a + b

async def multiply(ctx: Context, a: int, b: int) -> int:
    """Multiply two numbers."""
    # update our count
    cur_state = await ctx.get("state")
    cur_state["num_fn_calls"] += 1
    await ctx.set("state", cur_state)

    return a * b

...

workflow = AgentWorkflow(
    agents=[multiply_agent, addition_agent],
    root_agent="multiply_agent"
    initial_state={"num_fn_calls": 0},
    state_prompt="Current state: {state}. User message: {msg}",
)

# ä½¿ç”¨ä¸Šä¸‹æ–‡è¿è¡Œå·¥ä½œæµç¨‹
ctx = Context(workflow)
response = await workflow.run(user_msg="Can you add 5 and 3?", ctx=ctx)

# æ‹‰å‡ºå¹¶æ£€æŸ¥çŠ¶æ€
state = await ctx.get("state")
print(state["num_fn_calls"])
```

æ­å–œï¼æ‚¨ç°åœ¨å·²ç»æŒæ¡äº† LlamaIndex ä¸­ Agent çš„åŸºç¡€çŸ¥è¯†ï¼ğŸ‰

è®©æˆ‘ä»¬ç»§ç»­è¿›è¡Œæœ€åä¸€æ¬¡æµ‹éªŒæ¥å·©å›ºæ‚¨çš„çŸ¥è¯†ï¼ğŸš€
